### **논문 제목: Reference-based Metrics Disprove Themselves in Question Generation (A.K.A: NACo)**

---
#### **1. 논문 핵심 요약**

* **문제 제기:** 현재 질문 생성(QG) 분야에서 널리 사용되는 BLEU, BERTScore와 같은 참조 기반(Reference-based) 평가 지표들은 심각한 결함을 가지고 있습니다. 이 지표들은 AI가 만든 질문이 사람이 만든 '정답 질문(reference)'과 얼마나 유사한 단어를 포함하는지만 보기 때문에, 질문의 형태가 아니거나, 답변이 불가능한 엉터리 질문도 단순히 단어가 비슷하다는 이유로 높게 평가하는 경우가 많습니다. 이 문제는 대부분의 벤치마크가 단 하나의 정답 질문만 제공하기에 더욱 심각합니다.
* **핵심 주장 및 해결책:** 본 논문은 참조 기반 지표의 이러한 '무능함(incompetency)'을 증명하고, 그 대안으로 LLM을 활용한 새로운 참조 미기반(reference-free) 평가 지표 **NACo**를 제안합니다. NACo는 정답 질문과의 단어 비교가 아닌, 질문의 본질적인 품질인 **자연스러움(Naturalness), 답변 가능성(Answerability), 복잡성(Complexity)**이라는 3가지 다차원적 기준(multi-dimensional criteria)으로 평가합니다.
* **증명 방법:** 저자들은 기존 벤치마크의 제작 과정을 그대로 따라 하여, 사람이 직접 만든 '두 번째 정답 질문'을 추가로 수집했습니다. 그 결과, 기존 참조 기반 지표들이 이 새로운 (완벽히 올바른) 질문에 대해 원래 정답과 다르다는 이유만으로 매우 낮은 점수를 주는 것을 보여줌으로써 스스로의 평가 기준이 잘못되었음을 증명했습니다.
* **결과:** 실험 결과, NACo는 기존 지표들과 달리 좋은 질문과 결함이 있는 질문을 성공적으로 구별했으며, 사람의 실제 판단과 가장 높은 수준의 상관관계를 달성했습니다.

---
#### **2. 내 연구와의 관련점**

* **LLM을 활용한 평가 지표의 정당성 확보:** 본 논문은 참조 기반 지표의 한계를 명확한 실험으로 증명하고, 그 대안으로 LLM을 활용한 평가 방식(NACo)의 우수성을 주장합니다. 이는 내 연구에서 LLM을 평가자로 사용하는 접근법에 대한 강력한 이론적 근거 및 선행 연구로 활용될 수 있습니다.
* **구체적인 평가 기준(Criteria) 설계 방식:** '자연스러움(Naturalness)'을 평가할 때 문법 오류(grammar error) 여부를 포함하는 등, 추상적인 평가 기준을 구체적인 하위 항목으로 정의하는 방식을 제시합니다. 이는 내 연구에서 평가 지표를 설계할 때 참고할 수 있는 구체적인 방법론입니다.
* **기존 방법론과의 체계적인 비교 분석:** 기존의 참조 기반(BERTScore) 및 참조 미기반(RQUGE) 지표들의 약점을 구체적인 사례 연구(case study)를 통해 비판적으로 분석합니다. 이는 내 연구가 기존 방법론들과 어떻게 차별화되는지 보여주는 효과적인 서술 방식을 제공합니다.

---
#### **3. 인용하고 싶은 문구들 (페이지 번호)**

* "좋은 평가 지표라면, 사람이 검증한 질문에 대해 (기계가) 생성한 질문보다 나쁜 점수를 주지 않아야 한다."
* "QG 모델은 참조 질문과 비슷한 단어를 많이 사용하면서도 질문의 필수적인 요소는 무시하는 방식으로 평가 지표를 '속일' 수 있다."
* "참조 기반 지표의 이러한 무능함(incompetency)은, BART 모델이 참조 질문에 사용될 단어를 식별할 수는 있지만, 정작 일관성 있고 답변 가능한 질문을 만드는 데는 실패하기 때문에 발생한다."
* "NACo는 모든 상관관계 지표에서 질문의 전반적인 품질에 대한 사람의 판단과 가장 강력한 일치성을 보여준다."

---
#### **4. 비판하고 싶은 점들 **

* **'참조 미기반'의 불완전성:** NACo는 참조 미기반(reference-free)을 표방하지만, '복잡성' 점수를 산출하기 위해 데이터셋의 '기대 복잡도(expected complexity)'를 계산해야 합니다. 이 과정에서 결국 학습 데이터셋의 일부 참조 질문(references)을 샘플링해야 하므로, 참조로부터 완전히 자유롭지 못하다는 내재적 모순을 가집니다.
* **평가자의 성능 의존성:** NACo의 성능은 기반이 되는 QA 모델(LLM)의 성능에 크게 좌우됩니다. 논문에서도 GPT-3.5가 수학적 추론에 취약하여 올바른 질문을 잘못 평가한 사례를 직접 보여줍니다. 즉, 평가의 객관성이 기반 LLM의 능력에 종속되는 한계가 있습니다.
* **평가 기준 가중치의 임의성:** 최종 NACo 점수를 계산할 때 세 기준(자연스러움, 답변 가능성, 복잡성)에 대해 각각 '공평하게' 1/3의 가중치를 부여했습니다. 하지만 이 가중치가 왜 최적인지에 대한 탐구나 데이터셋의 특성에 따라 가중치를 다르게 적용하는 방안에 대한 논의는 부족합니다.

---
#### **5. 이해못한 것들**

* **실험 설계의 구체적인 규모:** 사람의 판단과 비교하는 실험에서 사용된 데이터의 정확한 규모(HotpotQA 96개 예제, 각 4개 질문)와 평가자 수(3명)를 명확히 파악하고, 이것이 통계적 신뢰도를 확보하기에 충분한 규모인지 검토할 필요가 있습니다.
* **통계 분석의 상세 내용:** Figure 1의 정규화된 값(Normalized Metric Value)이나 Table 2의 상관관계 계수(Pearson, Spearman 등)가 정확히 어떤 데이터와 계산 과정을 통해 산출되었는지 상세히 확인할 필요가 있습니다.
* **경쟁 모델과의 비교 방식:** 2장에서 기존 평가 지표(RQUGE 등)의 한계를 지적하는 논리 전개 방식을 보다 상세히 분석하여, 내 연구의 서론 및 관련 연구 섹션 구성에 활용할 필요가 있습니다.